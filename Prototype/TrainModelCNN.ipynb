{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Globals"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Use GPU\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "\n",
    "CLASS_LABELS = ['non-violent','violent']\n",
    "LABELS_COUNT = len(CLASS_LABELS)\n",
    "\n",
    "DATASET_PATH = 'Dataset/violent_keypoints_dataset.csv'\n",
    "CONF_DATASET_PATH = 'Dataset/violent_keypoints_conf_dataset.csv'\n",
    "\n",
    "MEAN_CONF_THRESHOLD = 0.3\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Import Dataset\n",
    "data = pd.read_csv(DATASET_PATH).values\n",
    "\n",
    "data[data == \"non-violent\"] = 0\n",
    "data[data == \"violent\"] = 1\n",
    "\n",
    "data = data.astype('float32')\n",
    "data_no_conf = data\n",
    "\n",
    "print(data.shape)\n",
    "print(\"-----DATA-----\")\n",
    "print(\"Non-Violent: \" + str(np.count_nonzero(data[:,0] == 0)))\n",
    "print(\"Violent: \" + str(np.count_nonzero(data[:,0] == 1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Confidence Scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Import Confidence\n",
    "conf_data = pd.read_csv(CONF_DATASET_PATH).values\n",
    "\n",
    "conf_data = conf_data[:, 1:]\n",
    "\n",
    "conf_data = conf_data.astype('float32')\n",
    "\n",
    "print(conf_data)\n",
    "print(conf_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Remove Data w/ Low Confidence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "high_conf_data = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "#Mean Confidence\n",
    "for x in range(len(conf_data)):\n",
    "    count += np.mean(conf_data[x])\n",
    "    if np.mean(conf_data[x]) >= MEAN_CONF_THRESHOLD:\n",
    "        high_conf_data.append(data[x])\n",
    "\n",
    "avg_mean = count / len(conf_data)\n",
    "print(\"AVG MEAN: \" + str(avg_mean))\n",
    "\n",
    "data = np.array(high_conf_data)\n",
    "\n",
    "\n",
    "print(data)\n",
    "print(data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TRAIN_AMOUNT = int(data.shape[0] * TRAIN_SPLIT)\n",
    "RANDOM_SEED = 89\n",
    "\n",
    "#Shuffle Data\n",
    "for x in range(100):\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "#Split Data into Train and Test\n",
    "train = data[0:TRAIN_AMOUNT]\n",
    "test = data[TRAIN_AMOUNT:]\n",
    "\n",
    "#Extract Data and Reshape to (3,38)\n",
    "x_train = np.reshape(train[:, 1:],(train.shape[0],19,3,2)).swapaxes(2,3).swapaxes(1,3)\n",
    "x_train = np.reshape(x_train,(train.shape[0],3,38), order=\"F\")\n",
    "x_test = np.reshape(test[:, 1:],(test.shape[0],19,3,2)).swapaxes(2,3).swapaxes(1,3)\n",
    "x_test = np.reshape(x_test,(test.shape[0],3,38), order=\"F\")\n",
    "\n",
    "#Extract Labels\n",
    "y_train = train[:,0]\n",
    "y_test = test[:,0]\n",
    "\n",
    "print(\"-----TRAIN-----\")\n",
    "print(\"Non-Violent: \" + str(np.count_nonzero(y_train == 0)))\n",
    "print(\"Violent: \" + str(np.count_nonzero(y_train == 1)))\n",
    "\n",
    "print(\"-----TEST-----\")\n",
    "print(\"Non-Violent: \" + str(np.count_nonzero(y_test == 0)))\n",
    "print(\"Violent: \" + str(np.count_nonzero(y_test == 1)))\n",
    "\n",
    "print(\"-----DATA-----\")\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build CNN Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_model = Sequential()\n",
    "\n",
    "my_model.add(Conv1D(64, 3, padding=\"same\", activation=\"relu\", input_shape=(3,38)))\n",
    "my_model.add(Conv1D(64, 3, padding=\"same\", activation=\"relu\"))\n",
    "\n",
    "my_model.add(Dropout(0.1))\n",
    "\n",
    "my_model.add(LSTM(38, return_sequences=True))\n",
    "\n",
    "my_model.add(MaxPooling1D(3, padding='same'))\n",
    "\n",
    "my_model.add(Flatten())\n",
    "\n",
    "my_model.add(Dropout(0.1))\n",
    "\n",
    "my_model.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "my_model.add(Dropout(0.5))\n",
    "\n",
    "my_model.add(Dense(1, activation=\"sigmoid\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "my_model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=['accuracy'])\n",
    "\n",
    "#Patience Callback\n",
    "patienceCallback = [tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5, restore_best_weights=True)]\n",
    "\n",
    "#Train Model\n",
    "history = my_model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data = (x_test, y_test), callbacks=patienceCallback)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Evaluate Model\n",
    "score = my_model.evaluate(x_test, y_test, batch_size=EPOCHS)\n",
    "\n",
    "my_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot Graphs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "xepochs = [i+1 for i in range(0, len(history.history['loss']))]\n",
    "plt.figure(figsize=(5,3))\n",
    "\n",
    "# Loss\n",
    "plt.plot(xepochs, history.history['loss'])\n",
    "plt.plot(xepochs, history.history['val_loss'])\n",
    "plt.xticks(xepochs)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(xepochs, history.history['accuracy'])\n",
    "plt.plot(xepochs, history.history['val_accuracy'])\n",
    "plt.xticks(xepochs)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification Report"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Shuffle Data\n",
    "for x in range(100):\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    np.random.shuffle(data_no_conf)\n",
    "\n",
    "test = data_no_conf[TRAIN_AMOUNT:]\n",
    "\n",
    "x_test = np.reshape(test[:, 1:],(test.shape[0],19,3,2)).swapaxes(2,3).swapaxes(1,3)\n",
    "x_test = np.reshape(x_test,(test.shape[0],3,38), order=\"F\")\n",
    "\n",
    "y_test = test[:,0]\n",
    "\n",
    "predictions = my_model.predict(x_test)\n",
    "\n",
    "pred_class = (predictions > 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_test, pred_class, target_names=['non-violent', 'violent']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Confusion Matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, pred_class)\n",
    "print(cm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Export Model and Weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# serialize to JSON\n",
    "json_file = my_model.to_json()\n",
    "with open(\"ViolentModel/CNN/violent_model_json\", \"w\") as file:\n",
    "   file.write(json_file)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "my_model.save_weights(\"ViolentModel/CNN/violent_model_weights.h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}