{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tkinter as tk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Globals & Prepare Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "Object Detection\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Import Object Detection Model Parameters\n",
    "OBJ_config = \"Model/yolov4.cfg\"\n",
    "OBJ_weights = \"Model/yolov4.weights\"\n",
    "OBJ_class_names_file = \"Model/coco.names\"\n",
    "\n",
    "# read class names from text file\n",
    "OBJ_class_names = None\n",
    "with open(OBJ_class_names_file, 'r') as f:\n",
    "    OBJ_class_names = [line.strip() for line in f.readlines()]\n",
    "\n",
    "OBJ_threshold = 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 107,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "Object Tracking\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "TRACK_list = cv2.legacy.MultiTracker_create()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 108,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "Pose Estimation\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "#Import Pose Estimation Model Parameters\n",
    "POSE_proto = \"Model/pose_deploy_linevec.prototxt\"\n",
    "POSE_weights = \"Model/pose_iter_440000.caffemodel\"\n",
    "\n",
    "POSE_body_parts = { \"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "               \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "               \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n",
    "               \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18 }\n",
    "\n",
    "POSE_part_pairs = [ [\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
    "               [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"],\n",
    "               [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"], [\"Neck\", \"LHip\"],\n",
    "               [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"], [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"],\n",
    "               [\"REye\", \"REar\"], [\"Nose\", \"LEye\"], [\"LEye\", \"LEar\"] ]\n",
    "\n",
    "POSE_threshold = 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Create Models\n",
    "OBJ_net = cv2.dnn.readNet(OBJ_config, OBJ_weights)\n",
    "POSE_net = cv2.dnn.readNetFromCaffe(POSE_proto, POSE_weights)\n",
    "\n",
    "#Set Models to use GPU\n",
    "OBJ_net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "OBJ_net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "POSE_net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "POSE_net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 110,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataSet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "DATASET_VIDEOS = \"Dataset/videos/\"\n",
    "DATASET_CLASSES = [\"violent\", \"non-violent\"]\n",
    "DATASET_CAMS = [\"/cam1/\",\"/cam2/\"]\n",
    "DATASET_EXP_LOC = \"Dataset/violent_keypoints_dataset.csv\"\n",
    "DATASET_EXP_LOC_CONF = \"Dataset/violent_keypoints_conf_dataset.csv\"\n",
    "DATASET_EXP_LOC_LOW = \"Dataset/violent_keypoints_low_dataset.csv\"\n",
    "DATASET_EXP_LOC_LOW_CONF = \"Dataset/violent_keypoints_low_conf_dataset.csv\"\n",
    "\n",
    "\n",
    "DATASET_COL = [\n",
    "                    \"Class\",\n",
    "                    \"Nose_X_1\", \"Nose_Y_1\", \"Nose_X_2\", \"Nose_Y_2\", \"Nose_X_3\", \"Nose_Y_3\",\n",
    "                    \"Neck_X_1\", \"Neck_Y_1\", \"Neck_X_2\", \"Neck_Y_2\", \"Neck_X_3\", \"Neck_Y_3\",\n",
    "                    \"RShoulder_X_1\", \"RShoulder_Y_1\", \"RShoulder_X_2\", \"RShoulder_Y_2\", \"RShoulder_X_3\", \"RShoulder_Y_3\",\n",
    "                    \"RElbow_X_1\", \"RElbow_Y_1\", \"RElbow_X_2\", \"RElbow_Y_2\", \"RElbow_X_3\", \"RElbow_Y_3\",\n",
    "                    \"RWrist_X_1\", \"RWrist_Y_1\", \"RWrist_X_2\", \"RWrist_Y_2\", \"RWrist_X_3\", \"RWrist_Y_3\",\n",
    "                    \"LShoulder_X_1\", \"LShoulder_Y_1\", \"LShoulder_X_2\", \"LShoulder_Y_2\", \"LShoulder_X_3\", \"LShoulder_Y_3\",\n",
    "                    \"LElbow_X_1\", \"LElbow_Y_1\", \"LElbow_X_2\", \"LElbow_Y_2\", \"LElbow_X_3\", \"LElbow_Y_3\",\n",
    "                    \"LWrist_X_1\", \"LWrist_Y_1\", \"LWrist_X_2\", \"LWrist_Y_2\", \"LWrist_X_3\", \"LWrist_Y_3\",\n",
    "                    \"RHip_X_1\", \"RHip_Y_1\", \"RHip_X_2\", \"RHip_Y_2\", \"RHip_X_3\", \"RHip_Y_3\",\n",
    "                    \"RKnee_X_1\", \"RKnee_Y_1\", \"RKnee_X_2\", \"RKnee_Y_2\", \"RKnee_X_3\", \"RKnee_Y_3\",\n",
    "                    \"RAnkle_X_1\", \"RAnkle_Y_1\", \"RAnkle_X_2\", \"RAnkle_Y_2\", \"RAnkle_X_3\", \"RAnkle_Y_3\",\n",
    "                    \"LHip_X_1\", \"LHip_Y_1\", \"LHip_X_2\", \"LHip_Y_2\", \"LHip_X_3\", \"LHip_Y_3\",\n",
    "                    \"LKnee_X_1\", \"LKnee_Y_1\", \"LKnee_X_2\", \"LKnee_Y_2\", \"LKnee_X_3\", \"LKnee_Y_3\",\n",
    "                    \"LAnkle_X_1\", \"LAnkle_Y_1\", \"LAnkle_X_2\", \"LAnkle_Y_2\", \"LAnkle_X_3\", \"LAnkle_Y_3\",\n",
    "                    \"REye_X_1\", \"REye_Y_1\", \"REye_X_2\", \"REye_Y_2\", \"REye_X_3\", \"REye_Y_3\",\n",
    "                    \"LEye_X_1\", \"LEye_Y_1\", \"LEye_X_2\", \"LEye_Y_2\", \"LEye_X_3\", \"LEye_Y_3\",\n",
    "                    \"REar_X_1\", \"REar_Y_1\", \"REar_X_2\", \"REar_Y_2\", \"REar_X_3\", \"REar_Y_3\",\n",
    "                    \"LEar_X_1\", \"LEar_Y_1\", \"LEar_X_2\", \"LEar_Y_2\", \"LEar_X_3\", \"LEar_Y_3\",\n",
    "                    \"Background_X_1\", \"Background_Y_1\", \"Background_X_2\", \"Background_Y_2\", \"Background_X_3\", \"Background_Y_3\"\n",
    "                    ]\n",
    "\n",
    "DATASET_COL_CONF = [\n",
    "                        \"Class\",\n",
    "                        \"Nose_1\", \"Nose_2\", \"Nose_3\",\n",
    "                        \"Neck_1\", \"Neck_2\", \"Neck_3\",\n",
    "                        \"RShoulder_1\", \"RShoulder_2\", \"RShoulder_3\",\n",
    "                        \"RElbow_1\", \"RElbow_2\", \"RElbow_3\",\n",
    "                        \"RWrist_1\", \"RWrist_2\", \"RWrist_3\",\n",
    "                        \"LShoulder_1\", \"LShoulder_2\", \"LShoulder_3\",\n",
    "                        \"LElbow_1\", \"LElbow_2\", \"LElbow_3\",\n",
    "                        \"LWrist_1\", \"LWrist_2\", \"LWrist_3\",\n",
    "                        \"RHip_1\", \"RHip_2\", \"RHip_3\",\n",
    "                        \"RKnee_1\", \"RKnee_2\", \"RKnee_3\",\n",
    "                        \"RAnkle_1\", \"RAnkle_2\", \"RAnkle_3\",\n",
    "                        \"LHip_1\", \"LHip_2\", \"LHip_3\",\n",
    "                        \"LKnee_1\", \"LKnee_2\", \"LKnee_3\",\n",
    "                        \"LAnkle_1\", \"LAnkle_2\", \"LAnkle_3\",\n",
    "                        \"REye_1\", \"REye_2\", \"REye_3\",\n",
    "                        \"LEye_1\", \"LEye_2\", \"LEye_3\",\n",
    "                        \"REar_1\", \"REar_2\", \"REar_3\",\n",
    "                        \"LEar_1\", \"LEar_2\", \"LEar_3\",\n",
    "                        \"Background_1\", \"Background_2\", \"Background_3\"\n",
    "\n",
    "\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 111,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Labelling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "global window\n",
    "global check_list\n",
    "global check_var_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Detect People"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "def get_output_layers(net):\n",
    "\n",
    "    layer_names = net.getLayerNames()\n",
    "\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    return output_layers\n",
    "\n",
    "def detect_individuals_from_image(OBJ_img):\n",
    "\n",
    "    OBJ_width = OBJ_img.shape[1]\n",
    "    OBJ_height = OBJ_img.shape[0]\n",
    "    OBJ_scale = 0.00392\n",
    "\n",
    "    OBJ_blob = cv2.dnn.blobFromImage(OBJ_img, OBJ_scale, (416, 416), (0,0,0), True, crop=False)\n",
    "\n",
    "    OBJ_net.setInput(OBJ_blob)\n",
    "    outs = OBJ_net.forward(get_output_layers(OBJ_net))\n",
    "\n",
    "    # initialization\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    nms_threshold = 0.4\n",
    "\n",
    "    # for each detection from each output layer\n",
    "    # get the confidence, class id, bounding box params\n",
    "    # and ignore weak detections (confidence < 0.5)\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > OBJ_threshold:\n",
    "                center_x = int(detection[0] * OBJ_width)\n",
    "                center_y = int(detection[1] * OBJ_height)\n",
    "                w = int(detection[2] * OBJ_width)\n",
    "                h = int(detection[3] * OBJ_height)\n",
    "                x = center_x - w / 2\n",
    "                y = center_y - h / 2\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([x, y, w, h])\n",
    "\n",
    "    #Apply non-max suppression\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, OBJ_threshold, nms_threshold)\n",
    "\n",
    "    confident_people_box = []\n",
    "\n",
    "    for i in indices:\n",
    "        #Only return if detected object is a Person\n",
    "        if class_ids[i] == 0:\n",
    "            confident_people_box.append(boxes[i])\n",
    "\n",
    "    return confident_people_box"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Track People"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "def start_tracking_from_boxes(TRACK_BBoxes, TRACK_img):\n",
    "\n",
    "    #New Empty Track list\n",
    "    new_TRACK_list = cv2.legacy.MultiTracker_create()\n",
    "\n",
    "    #Fill new Track list with the new Bounding boxes\n",
    "    for TRACK_box in TRACK_BBoxes:\n",
    "\n",
    "        tracker = cv2.legacy.TrackerCSRT_create()\n",
    "        new_TRACK_list.add(tracker, TRACK_img, TRACK_box)\n",
    "\n",
    "    global TRACK_list\n",
    "    TRACK_list = new_TRACK_list\n",
    "\n",
    "def track_using_trackers(TRACK_img, FINAL_img):\n",
    "\n",
    "    global TRACK_list\n",
    "\n",
    "    # grab the updated bounding box coordinates (if any) for each object that is being tracked\n",
    "    (success, TRACK_boxes) = TRACK_list.update(TRACK_img)\n",
    "\n",
    "def expand_tracking_box(image, FINAL_img):\n",
    "\n",
    "    global TRACK_list\n",
    "    TRACK_boxes = TRACK_list.getObjects()\n",
    "    TRACK_expanded_boxes = [[] for i in range(len(TRACK_boxes))]\n",
    "\n",
    "    new_OBJ_boxes = detect_individuals_from_image(image)\n",
    "\n",
    "    # loop over the bounding boxes and draw them on the frame\n",
    "    for ibox in range(len(TRACK_boxes)):\n",
    "\n",
    "        x = int(TRACK_boxes[ibox][0])\n",
    "        y = int(TRACK_boxes[ibox][1])\n",
    "        w = int(TRACK_boxes[ibox][2])\n",
    "        h = int(TRACK_boxes[ibox][3])\n",
    "\n",
    "        cv2.rectangle(FINAL_img, (x,y), (x+w,y+h), (255,0,0), 2)\n",
    "        cv2.putText(FINAL_img, \"Tracker \"+str(ibox), (x+10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "    #Take account of tracking boxes\n",
    "    for TRACK_box_i in range(len(TRACK_boxes)):\n",
    "        b_box = TRACK_boxes[TRACK_box_i]\n",
    "        TRACK_expanded_boxes[TRACK_box_i].append((int(b_box[0]),int(b_box[1]),int(b_box[2]),int(b_box[3])))\n",
    "\n",
    "    #Take account new object detection boxes\n",
    "    for OBJ_box_i in range(len(new_OBJ_boxes)):\n",
    "        new_OBJ_Center = get_center_of_box(new_OBJ_boxes[OBJ_box_i])\n",
    "\n",
    "        distance = sys.maxsize\n",
    "        trackerIndex = -1\n",
    "\n",
    "        for TRACK_box_i in range(len(TRACK_boxes)):\n",
    "            curr_TRACK_Center = get_center_of_box(TRACK_boxes[TRACK_box_i])\n",
    "\n",
    "            curr_distance = math.dist(curr_TRACK_Center, new_OBJ_Center)\n",
    "\n",
    "            if curr_distance < distance:\n",
    "                distance = curr_distance\n",
    "                trackerIndex = TRACK_box_i\n",
    "\n",
    "        b_box = new_OBJ_boxes[OBJ_box_i]\n",
    "        TRACK_expanded_boxes[trackerIndex].append((int(b_box[0]),int(b_box[1]),int(b_box[2]),int(b_box[3])))\n",
    "\n",
    "    return TRACK_expanded_boxes\n",
    "\n",
    "def get_center_of_box(boundingBox):\n",
    "\n",
    "    boundingBox_Xcenter = int((boundingBox[0] + (boundingBox[0] + boundingBox[2])) / 2)\n",
    "    boundingBox_Ycenter = int((boundingBox[1] + (boundingBox[1] + boundingBox[3])) / 2)\n",
    "\n",
    "    return(boundingBox_Xcenter, boundingBox_Ycenter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Estimate Human Poses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "def seperate_person(person_BB, image, finalImage):\n",
    "\n",
    "    POSE_IMG = np.zeros(finalImage.shape, dtype=np.uint8)\n",
    "\n",
    "    for curr_BB in person_BB:\n",
    "\n",
    "        tl = curr_BB[0]\n",
    "        tr = curr_BB[0]+curr_BB[2]\n",
    "        bl = curr_BB[1]\n",
    "        br = curr_BB[1]+curr_BB[3]\n",
    "\n",
    "        #Seperate each product/contour into a new image by cropping the image to the bounding box of the product\n",
    "        POSE_IMG[bl:br, tl:tr] = image[bl:br, tl:tr]\n",
    "\n",
    "    return POSE_IMG\n",
    "\n",
    "def get_human_pose_from_img(POSE_img_box, FINAL_img):\n",
    "\n",
    "    IMG_HEIGHT = POSE_img_box.shape[0]\n",
    "    IMG_WIDTH = POSE_img_box.shape[1]\n",
    "\n",
    "    #Resize for prediction\n",
    "    BLOB_HEIGHT=368\n",
    "    BLOB_WIDTH=int((BLOB_HEIGHT/IMG_HEIGHT)*IMG_WIDTH)\n",
    "\n",
    "    # Use the given image as input, which needs to be blob(s).\n",
    "    imgBlob = cv2.dnn.blobFromImage(POSE_img_box, 1.0/255, (BLOB_WIDTH, BLOB_HEIGHT), (0,0,0), swapRB=True, crop=False)\n",
    "    POSE_net.setInput(imgBlob)\n",
    "\n",
    "    # Runs a forward pass to compute the POSE_MODEL output\n",
    "    out = POSE_net.forward()\n",
    "    # MobilePOSE_MODEL output [1, 57, -1, -1], we only need the first 19 elements\n",
    "    out = out[:, :19, :, :]\n",
    "\n",
    "    assert(len(POSE_body_parts) == out.shape[1])\n",
    "\n",
    "    points = []\n",
    "    points_conf = []\n",
    "    points_low = []\n",
    "    points_low_conf = []\n",
    "\n",
    "    for i in range(len(POSE_body_parts)):\n",
    "        # Slice heatmap of corresponding body's part.\n",
    "        heatMap = out[0, i, :, :]\n",
    "\n",
    "        # Originally, we try to find all the local maximums. To simplify a sample\n",
    "        # we just find a global one. However only a single pose at the same time\n",
    "        # could be detected this way.\n",
    "        _, conf, _, point = cv2.minMaxLoc(heatMap)\n",
    "        x = (IMG_WIDTH * point[0]) / out.shape[3]\n",
    "        y = (IMG_HEIGHT * point[1]) / out.shape[2]\n",
    "\n",
    "        # Add a point if it's confidence is higher than THRESHOLD.\n",
    "        points.append((int(x), int(y)) if conf > POSE_threshold else (0,0))\n",
    "        points_conf.append(conf if conf > POSE_threshold else 0.0)\n",
    "\n",
    "        # A low confidence variant\n",
    "        points_low.append((int(x), int(y)))\n",
    "        points_low_conf.append(conf)\n",
    "\n",
    "    for pair in POSE_part_pairs:\n",
    "        partFrom = pair[0]\n",
    "        partTo = pair[1]\n",
    "        assert(partFrom in POSE_body_parts)\n",
    "        assert(partTo in POSE_body_parts)\n",
    "\n",
    "        idFrom = POSE_body_parts[partFrom]\n",
    "        idTo = POSE_body_parts[partTo]\n",
    "\n",
    "        if not points[idFrom] == (0,0) and not points[idTo] == (0,0):\n",
    "            cv2.putText(FINAL_img,partFrom,points[idFrom], cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(FINAL_img,partTo,points[idTo],cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 1, cv2.LINE_AA)\n",
    "            cv2.line(FINAL_img, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
    "            cv2.ellipse(FINAL_img, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv2.FILLED)\n",
    "            cv2.ellipse(FINAL_img, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv2.FILLED)\n",
    "\n",
    "    return points, points_conf, points_low, points_low_conf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Labelling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "def create_labelling_window(list_of_persons):\n",
    "\n",
    "    global window\n",
    "    global check_list\n",
    "    global check_var_list\n",
    "\n",
    "    window = tk.Tk()\n",
    "    window.title(\"Labelling\")\n",
    "    window.rowconfigure(0, minsize=100, weight=1)\n",
    "    window.columnconfigure(1, minsize=70, weight=1)\n",
    "    window.resizable(False, False)\n",
    "\n",
    "    fr_checks = tk.Frame(window)\n",
    "    fr_btn = tk.Frame(window)\n",
    "\n",
    "    check_list = []\n",
    "    check_var_list = []\n",
    "\n",
    "    for x in range(len(list_of_persons)):\n",
    "\n",
    "        check_var_list.append(tk.IntVar())\n",
    "\n",
    "        check_list.append(tk.Checkbutton(fr_checks, text = \"Person \" + str(x), variable = check_var_list[x], onvalue = 1, offvalue = 0, height = 2, width = 10))\n",
    "\n",
    "        check_list[x].grid(row=x , column=0 , sticky=\"ew\" , padx=5 , pady=5)\n",
    "\n",
    "    btn_ok = tk.Button(fr_btn, text=\"OK\", command=window.destroy)\n",
    "\n",
    "    fr_checks.grid(row=0, column=0, sticky=\"ns\")\n",
    "    fr_btn.grid(row=1, column=0, sticky=\"ns\")\n",
    "    btn_ok.grid(row=0, column=0, sticky=\"ew\", padx=5, pady=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Data for export"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "def format_data(coord_loc_list, frame_shape):\n",
    "\n",
    "    global check_var_list\n",
    "\n",
    "    csv_list = []\n",
    "\n",
    "    for x in range(len(coord_loc_list)):\n",
    "        for y in range(0, len(coord_loc_list[x]),3):\n",
    "            try:\n",
    "                list_output_keypoints = []\n",
    "                for z in range(len(coord_loc_list[x][y])):\n",
    "                    for frame_count in range(3):\n",
    "                        for coord_count in range(2):\n",
    "                            list_output_keypoints.append( (coord_loc_list[x][y + frame_count][z][coord_count]) / frame_shape [coord_count])\n",
    "\n",
    "                csv_list.append([(\"violent\" if check_var_list[x].get() == 1 else \"non-violent\")] + list_output_keypoints)\n",
    "            except:\n",
    "                break\n",
    "\n",
    "\n",
    "    return csv_list\n",
    "\n",
    "def format_conf_data(conf_list):\n",
    "\n",
    "    global check_var_list\n",
    "\n",
    "    csv_list = []\n",
    "\n",
    "    for person_i in range(len(conf_list)):\n",
    "        for frame_i in range(0, len(conf_list[person_i]),3):\n",
    "            try:\n",
    "                list_output_keypoints = []\n",
    "                for joint_i in range(len(conf_list[person_i][frame_i])):\n",
    "                    for frame_count in range(3):\n",
    "                        list_output_keypoints.append(conf_list[person_i][frame_i + frame_count][joint_i])\n",
    "\n",
    "                csv_list.append([(\"violent\" if check_var_list[person_i].get() == 1 else \"non-violent\")] + list_output_keypoints)\n",
    "            except:\n",
    "                break\n",
    "\n",
    "    return csv_list\n",
    "\n",
    "\n",
    "def cull_data(coord_loc_list, conf_list):\n",
    "    removed_dub_list = []\n",
    "    removed_dub_list_conf = []\n",
    "\n",
    "    for x in range(len(coord_loc_list)):\n",
    "        if coord_loc_list[x] not in removed_dub_list:\n",
    "            removed_dub_list.append(coord_loc_list[x])\n",
    "            removed_dub_list_conf.append(conf_list[x])\n",
    "\n",
    "    return removed_dub_list, removed_dub_list_conf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Export to CSV"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "def export_to_csv(keypoints_list, csv_col, export_loc):\n",
    "\n",
    "    np_array_rows = np.array(keypoints_list)\n",
    "    np_array_columns = np.array(csv_col)\n",
    "\n",
    "    df = pd.DataFrame(np_array_rows)\n",
    "\n",
    "    if not os.path.isfile(export_loc):\n",
    "       df.to_csv(export_loc, index=False, header=np_array_columns)\n",
    "    else: # else it exists so append without writing the header\n",
    "       df.to_csv(export_loc, index=False, mode='a', header=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "def num_sort(received_string):\n",
    "    return list(map(int, re.findall(r'\\d+', received_string)))[0]\n",
    "\n",
    "def DataExtraction(VIDEO_LOC):\n",
    "\n",
    "    global window\n",
    "    global check_list\n",
    "    global check_var_list\n",
    "\n",
    "    frameCount = 0\n",
    "    OBJ_boxes = []\n",
    "\n",
    "    poseLoc = []\n",
    "    poseLoc_conf = []\n",
    "    poseLoc_low = []\n",
    "    poseLoc_low_conf = []\n",
    "\n",
    "    video_reso = 0\n",
    "\n",
    "    #Initialize the video stream\n",
    "    MEDIA_RAW = cv2.VideoCapture(VIDEO_LOC)\n",
    "\n",
    "    video_reso = (MEDIA_RAW.get(3),MEDIA_RAW.get(4))\n",
    "\n",
    "    #Loop over the frames from the video stream\n",
    "    while True:\n",
    "\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if k == 27:\n",
    "            MEDIA_RAW.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            return True\n",
    "            break\n",
    "\n",
    "        #Grab the frame from the threaded video stream\n",
    "        hasFrame, image = MEDIA_RAW.read()\n",
    "\n",
    "        if not hasFrame:\n",
    "            break\n",
    "\n",
    "        finalImage = image.copy()\n",
    "        frameCount += 1\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        #Only Detect and Track People in the first frame\n",
    "        if frameCount == 1:\n",
    "            #Detect People in Image and return their Boxes Position\n",
    "            OBJ_boxes = detect_individuals_from_image(image)\n",
    "\n",
    "            #Pass Bounding boxes to Tracker to start tracking\n",
    "            start_tracking_from_boxes(OBJ_boxes, image)\n",
    "\n",
    "            #Prepare Pose Save Locations\n",
    "            poseLoc = [[] for i in range(len(OBJ_boxes))]\n",
    "            poseLoc_conf = [[] for i in range(len(OBJ_boxes))]\n",
    "            poseLoc_low = [[] for i in range(len(OBJ_boxes))]\n",
    "            poseLoc_low_conf = [[] for i in range(len(OBJ_boxes))]\n",
    "\n",
    "            #Create Tkinter window\n",
    "            create_labelling_window(OBJ_boxes)\n",
    "\n",
    "        #Sync trackers with new frame\n",
    "        track_using_trackers(image, finalImage)\n",
    "\n",
    "        #Expand tracker bounding box with more accurate bounding boxes\n",
    "        POSE_EST_LOC = expand_tracking_box(image, finalImage)\n",
    "\n",
    "        #Apply Pose Estimation in Bounding Boxes of every person\n",
    "        for person_BBs_i in range(len(POSE_EST_LOC)):\n",
    "\n",
    "            #Create Image focusing on a single person\n",
    "            POSE_IMG = seperate_person(POSE_EST_LOC[person_BBs_i], image, finalImage)\n",
    "\n",
    "            pose_calc, pose_calc_conf, pose_calc_low, pose_calc_low_conf = get_human_pose_from_img(POSE_IMG, finalImage)\n",
    "\n",
    "            #Estimate Pose on a single person\n",
    "            poseLoc[person_BBs_i].append(pose_calc)\n",
    "            poseLoc_conf[person_BBs_i].append(pose_calc_conf)\n",
    "            poseLoc_low[person_BBs_i].append(pose_calc_low)\n",
    "            poseLoc_low_conf[person_BBs_i].append(pose_calc_low_conf)\n",
    "\n",
    "        #Resize for better output\n",
    "        finalImage = cv2.resize(finalImage, (int((700/finalImage.shape[0])*finalImage.shape[1]),700))\n",
    "\n",
    "        #Time taken\n",
    "        cv2.putText(finalImage, f'Time: {time.time() - start}', (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0))\n",
    "\n",
    "        #show the output frame\n",
    "        cv2.imshow(VIDEO_LOC, finalImage)\n",
    "\n",
    "        #Tkinter window running\n",
    "        window.mainloop()\n",
    "\n",
    "    #Stop any videos\n",
    "    MEDIA_RAW.release()\n",
    "\n",
    "    #Close all windows\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    #Format the data for export\n",
    "    formatted_data = format_data(poseLoc, video_reso)\n",
    "    formatted_data_conf = format_conf_data(poseLoc_conf)\n",
    "    formatted_data_low = format_data(poseLoc_low, video_reso)\n",
    "    formatted_data_low_conf = format_conf_data(poseLoc_low_conf)\n",
    "\n",
    "    #Remove any duplicate data\n",
    "    finalised_data, formatted_data_conf = cull_data(formatted_data, formatted_data_conf)\n",
    "    finalised_data_low, formatted_data_low_conf = cull_data(formatted_data_low, formatted_data_low_conf)\n",
    "\n",
    "    #Export to CSV\n",
    "    export_to_csv(finalised_data, DATASET_COL, DATASET_EXP_LOC)\n",
    "    export_to_csv(formatted_data_conf, DATASET_COL_CONF, DATASET_EXP_LOC_CONF)\n",
    "    export_to_csv(finalised_data_low, DATASET_COL, DATASET_EXP_LOC_LOW)\n",
    "    export_to_csv(formatted_data_low_conf, DATASET_COL_CONF, DATASET_EXP_LOC_LOW_CONF)\n",
    "\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Method"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_process = False\n",
    "\n",
    "#Go through each video in the dataset\n",
    "for dataset_class in DATASET_CLASSES:\n",
    "    for camera in DATASET_CAMS:\n",
    "\n",
    "        CURRENT_DIR = DATASET_VIDEOS + dataset_class + camera\n",
    "\n",
    "        list_video_names = listdir(CURRENT_DIR)\n",
    "        list_video_names.sort(key=num_sort)\n",
    "\n",
    "        #For each video run the pipeline\n",
    "        for video_name in list_video_names:\n",
    "\n",
    "            CURRENT_VID = CURRENT_DIR + video_name\n",
    "\n",
    "            end_process = DataExtraction(CURRENT_VID)\n",
    "\n",
    "            if end_process:\n",
    "                break\n",
    "            else:\n",
    "                #Move finished video to another location\n",
    "                shutil.move(CURRENT_VID, \"Dataset/videos_processed/\" + dataset_class + camera)\n",
    "\n",
    "        if end_process:\n",
    "            break\n",
    "    if end_process:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}